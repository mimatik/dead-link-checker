# Dead Link Checker

Python web crawler for detecting dead links on websites.

## Features

- ‚úÖ Crawls entire domain (all internal pages)
- ‚úÖ Checks all links (internal and external)
- ‚úÖ Detects dead links (404, 500, timeout, etc.)
- ‚úÖ Colored real-time console output
- ‚úÖ Exports results to CSV file with domain name
- ‚úÖ Configuration via YAML file
- ‚úÖ Smart error detection (ignores false positives)
- ‚úÖ Crawling progress statistics

## Quick Start

1. **Setup:**
```bash
git clone git@github.com:mimatik/dead-link-checker.git
cd dead-link-checker
curl -LsSf https://astral.sh/uv/install.sh | sh  # Install uv
uv sync  # Install dependencies
```

2. **Run via CLI:**
```bash
# Using a config file
uv run python cli.py crawl --config-id example

# Using a direct URL
uv run python cli.py crawl --url https://example.com

# List available configs
uv run python cli.py list-configs
```

3. **Run via API + Web UI:**

**Development mode (with hot reload):**
```bash
# Terminal 1: Start Flask API server (port 5555 kv≈Øli AirPlay na Macu)
uv run flask --app app --debug run --port 5555

# Terminal 2: Start Vite dev server
cd frontend
npm run dev
# Open http://localhost:5173
```

**Production mode (single server):**
```bash
# Build frontend
cd frontend && npm run build && cd ..

# Start Flask (serves API + built frontend)
uv run flask --app app run --port 5555
# Open http://localhost:5555
```

4. **Results:** `reports/dead_links_report_DOMAIN_TIMESTAMP.csv`

## Development

This project uses [uv](https://docs.astral.sh/uv/) for fast dependency management and Python environment handling.

### Project Structure

```
dead_link_crawler/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler.py      # Core crawling logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_store.py # Config CRUD operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jobs.py         # Job management
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py       # Flask API endpoints
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py         # Flask app factory
‚îú‚îÄ‚îÄ cli.py                  # CLI interface
‚îú‚îÄ‚îÄ custom_config_json/     # JSON configurations
‚îú‚îÄ‚îÄ reports/                # Generated CSV reports
‚îî‚îÄ‚îÄ .data/                  # Job state persistence
```

### Commands

**Backend:**
```bash
# Install Python dependencies
uv sync

# Run CLI
uv run python cli.py crawl --config-id example

# Run API server
uv run flask --app app run

# Run in development mode with auto-reload
uv run flask --app app --debug run

# Add new Python dependencies
uv add package-name

# Update Python dependencies
uv lock --upgrade
```

**Frontend:**
```bash
cd frontend

# Install dependencies (first time only)
npm install

# Run development server
npm run dev

# Build for production
npm run build

# Preview production build
npm run preview
```

## Configuration

Configurations are stored as JSON files in `custom_config_json/` directory.

### Creating a Configuration

Create a new JSON file in `custom_config_json/`:

```json
{
  "name": "My Website",
  "start_url": "https://mywebsite.com",
  "timeout": 15,
  "delay": 0.5,
  "max_depth": null,
  "output_dir": "reports",
  "show_skipped_links": false,
  "whitelist_codes": [403, 999],
  "domain_rules": {
    "linkedin.com": {
      "allowed_codes": [999, 429],
      "description": "LinkedIn rate limiting"
    },
    "twitter.com": {
      "allowed_codes": [403],
      "description": "Twitter access restriction"
    }
  }
}
```

### Configuration Options

- `start_url` (required): Starting URL for the crawl
- `timeout`: Request timeout in seconds (default: 15)
- `delay`: Delay between requests in seconds (default: 0.5)
- `max_depth`: Maximum crawl depth, null for unlimited (default: null)
- `output_dir`: Directory for reports (default: "reports")
- `show_skipped_links`: Show skipped links in output (default: false)
- `whitelist_codes`: HTTP status codes to treat as non-errors
- `domain_rules`: Domain-specific rules for handling certain sites

### Using Configurations

**CLI:**
```bash
uv run python cli.py crawl --config-id mywebsite
uv run python cli.py list-configs
uv run python cli.py show-config mywebsite
```

**API:**
```bash
# List configs
curl http://localhost:5000/api/configs

# Get config
curl http://localhost:5000/api/configs/mywebsite

# Start crawl with config
curl -X POST http://localhost:5000/api/crawl \
  -H "Content-Type: application/json" \
  -d '{"configId": "mywebsite"}'
```

## Output

```
üîç Crawling [1/3]: https://example.com
‚úì OK: https://example.com/about
‚úó ERROR [HTTP 404]: https://example.com/old-page
‚ö† WARNING [301]: https://example.com/redirect

üìä STATISTICS:
   Pages crawled: 15
   Links checked: 127
   Errors found: 3
```

## CSV Report

| Column | Description |
|--------|-------------|
| error_type | Error type (HTTP 404, Timeout, etc.) |
| link_url | URL of the problematic link |
| link_text | Link text |
| source_page | Page where the link is located |

## Web UI

Modern React-based web interface with:

- **Dashboard** - View recent jobs and their status with live updates
- **Configurations** - Create, edit, and manage crawl configurations
- **Run Crawl** - Select a configuration and start a new crawl
- **Reports** - Browse and download generated CSV reports

The UI automatically refreshes job statuses and provides real-time feedback.

## API Endpoints

### Configurations
- `GET /api/configs` - List all configurations
- `GET /api/configs/:id` - Get configuration by ID
- `POST /api/configs` - Create new configuration
- `PUT /api/configs/:id` - Update configuration
- `DELETE /api/configs/:id` - Delete configuration

### Jobs
- `POST /api/crawl` - Start new crawl job
- `GET /api/jobs` - List recent jobs
- `GET /api/jobs/:id` - Get job status and details

### Reports
- `GET /api/reports` - List available reports
- `GET /api/reports/:filename` - Download report file

## Tips

- **Large websites:** Use `delay: 1-2` seconds
- **Testing:** Set `max_depth: 2` for faster results
- **Interruption:** CTRL+C saves partial report (CLI only)

## License

MIT License - Free to use.